%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Christoph Garth at 2018-09-14 09:33:34 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{Wang18,
	Author = {F. Wang, I. Wald, Q. Wu, W. Usher, C. R. Johnson},
	Date-Added = {2018-09-14 09:32:52 +0200},
	Date-Modified = {2018-09-14 09:33:34 +0200},
	Journal = {IEEE Trans. Vis. Comput. Graph.},
	Title = {CPU Iso-surface Ray Tracing of Adaptive Mesh Refinement Data},
	Year = {to appear}}

@inproceedings{Wald17,
	Acmid = {3139305},
	Address = {New York, NY, USA},
	Articleno = {9},
	Author = {Wald, Ingo and Brownlee, Carson and Usher, Will and Knoll, Aaron},
	Booktitle = {SIGGRAPH Asia 2017 Symposium on Visualization},
	Date-Added = {2018-09-14 09:30:06 +0200},
	Date-Modified = {2018-09-14 09:30:07 +0200},
	Doi = {10.1145/3139295.3139305},
	Isbn = {978-1-4503-5411-0},
	Keywords = {Berger Colella AMR scheme, adaptive mesh refinement (AMR), ray tracing, volume ray tracing},
	Location = {Bangkok, Thailand},
	Numpages = {8},
	Pages = {9:1--9:8},
	Publisher = {ACM},
	Series = {SA '17},
	Title = {CPU Volume Rendering of Adaptive Mesh Refinement Data},
	Url = {http://doi.acm.org/10.1145/3139295.3139305},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3139295.3139305},
	Bdsk-Url-2 = {https://doi.org/10.1145/3139295.3139305}}

@article{Norman99,
	Author = {M. L. Norman and J. Shalf and S. Levy and G. Daues},
	Date-Added = {2018-09-14 09:29:40 +0200},
	Date-Modified = {2018-09-14 09:29:41 +0200},
	Doi = {10.1109/5992.774839},
	Issn = {1521-9615},
	Journal = {Computing in Science Engineering},
	Keywords = {cosmology;astronomy computing;data visualisation;mesh generation;tree data structures;digital simulation;data management;visualization strategies;adaptive mesh refinement simulations;cosmological applications;remote serving data;cosmological AMR algorithm;galaxy cluster formation;high resolution grids;stars;diffuse gas;local mesh refinement;global coarse grid;multiscale phenomena;Adaptive mesh refinement;Data visualization;Computational modeling;Clustering algorithms;Spatial resolution;Data structures;Animation;Navigation;Multidimensional systems;Numerical simulation},
	Month = {July},
	Number = {4},
	Pages = {36-47},
	Title = {Diving deep: data-management and visualization strategies for adaptive mesh refinement simulations},
	Volume = {1},
	Year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1109/5992.774839}}

@article{Berger89,
	Acmid = {1718335},
	Address = {San Diego, CA, USA},
	Author = {Berger, M. J. and Colella, P.},
	Date-Added = {2018-09-14 09:29:10 +0200},
	Date-Modified = {2018-09-14 09:29:11 +0200},
	Doi = {10.1016/0021-9991(89)90035-1},
	Issn = {0021-9991},
	Issue_Date = {May, 1989},
	Journal = {J. Comput. Phys.},
	Month = may,
	Number = {1},
	Numpages = {21},
	Pages = {64--84},
	Publisher = {Academic Press Professional, Inc.},
	Title = {Local Adaptive Mesh Refinement for Shock Hydrodynamics},
	Url = {http://dx.doi.org/10.1016/0021-9991(89)90035-1},
	Volume = {82},
	Year = {1989},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/0021-9991(89)90035-1}}

@inproceedings{Fabian11,
	Author = {N. Fabian and K. Moreland and D. Thompson and A. C. Bauer and P. Marion and B. Gevecik and M. Rasquin and K. E. Jansen},
	Booktitle = {2011 IEEE Symposium on Large Data Analysis and Visualization},
	Date-Added = {2018-09-14 09:27:23 +0200},
	Date-Modified = {2018-09-14 09:27:24 +0200},
	Doi = {10.1109/LDAV.2011.6092322},
	Keywords = {data analysis;data compression;data visualisation;feature extraction;multiprocessing systems;open systems;software libraries;ParaView coprocessing library;in situ visualization library;high performance computing;CPU capability;disk write speed;scientific simulation;data compression;interactive post-processing;readable data extraction;library scalability;Libraries;Pipelines;Data visualization;Data models;Adaptation models;Data mining;Sockets;coprocessing;in situ;simulation;scaling},
	Month = {Oct},
	Pages = {89-96},
	Title = {The ParaView Coprocessing Library: A scalable, general purpose in situ visualization library},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/LDAV.2011.6092322}}

@inproceedings{Whitlock11,
	Author = {Whitlock, Brad and Favre, Jean M. and Meredith, Jeremy S.},
	Booktitle = {Eurographics Symposium on Parallel Graphics and Visualization},
	Date-Added = {2018-09-14 09:26:46 +0200},
	Date-Modified = {2018-09-14 09:26:47 +0200},
	Doi = {10.2312/EGPGV/EGPGV11/101-109},
	Editor = {Torsten Kuhlen and Renato Pajarola and Kun Zhou},
	Isbn = {978-3-905674-32-3},
	Issn = {1727-348X},
	Publisher = {The Eurographics Association},
	Title = {{Parallel In Situ Coupling of Simulation with a Fully Featured Visualization System}},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.2312/EGPGV/EGPGV11/101-109}}

@inproceedings{Lofstead08,
	Acmid = {1383533},
	Address = {New York, NY, USA},
	Author = {Lofstead, Jay F. and Klasky, Scott and Schwan, Karsten and Podhorszki, Norbert and Jin, Chen},
	Booktitle = {Proceedings of the 6th International Workshop on Challenges of Large Applications in Distributed Environments},
	Date-Added = {2018-09-14 09:24:21 +0200},
	Date-Modified = {2018-09-14 09:24:23 +0200},
	Doi = {10.1145/1383529.1383533},
	Isbn = {978-1-60558-156-9},
	Keywords = {HDF-5, MPI-IO, modular IO, visualization, workflow},
	Location = {Boston, MA, USA},
	Numpages = {10},
	Pages = {15--24},
	Publisher = {ACM},
	Series = {CLADE '08},
	Title = {Flexible IO and Integration for Scientific Codes Through the Adaptable IO System (ADIOS)},
	Url = {http://doi.acm.org/10.1145/1383529.1383533},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1383529.1383533},
	Bdsk-Url-2 = {https://doi.org/10.1145/1383529.1383533}}

@inproceedings{Meng12,
	Author = {Q. Meng and A. Humphrey and M. Berzins},
	Booktitle = {2012 SC Companion: High Performance Computing, Networking Storage and Analysis},
	Date-Added = {2018-09-14 09:21:46 +0200},
	Date-Modified = {2018-09-14 09:21:48 +0200},
	Doi = {10.1109/SCC.2012.6674233},
	Keywords = {application program interfaces;data structures;message passing;multi-threading;processor scheduling;shared memory systems;Keeneland;TitanDev;DoE Jaguar XK6 system;adaptive mesh refinement;lock-free data structures;shared memory model on-node;asynchronous out-of-order scheduling;GPU computational tasks;hybrid runtime system;unified multithreaded MPI task scheduler design;coprocessors;on-node accelerators;CPU core counts;MPI-free user-coded tasks;structured adaptive grids;fluid-structure interaction problems;heterogeneous systems;asynchronous task execution;multithreaded runtime system;unified heterogeneous task scheduling;Uintah framework;Graphics processing units;Instruction sets;Runtime;Data warehouses;Computer architecture;Master-slave;Parallel processing},
	Month = {Nov},
	Pages = {2441-2448},
	Title = {The uintah framework: a unified heterogeneous task scheduling and runtime system},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/SCC.2012.6674233}}

@book{Eversen09,
	Author = {Geir Evensen},
	Date-Added = {2018-09-14 09:20:04 +0200},
	Date-Modified = {2018-09-14 09:21:09 +0200},
	Doi = {10.1007/978-3-642-03711-5},
	Publisher = {Springer},
	Title = {Data Assimilation -- The Ensemble Kalman Filter},
	Year = {2009}}

@article{Gunther15,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150300884G},
	Archiveprefix = {arXiv},
	Author = {{G{\"u}nther}, S. and {Gauger}, N.~R. and {Wang}, Q.},
	Date-Added = {2018-09-14 09:18:12 +0200},
	Date-Modified = {2018-09-14 09:18:14 +0200},
	Eprint = {1503.00884},
	Journal = {ArXiv e-prints},
	Keywords = {Mathematics - Optimization and Control},
	Month = mar,
	Primaryclass = {math.OC},
	Title = {{Simultaneous Single-Step One-Shot Optimization with Unsteady PDEs}},
	Year = 2015}

@webpage{XiCAM,
	Date-Added = {2018-09-14 09:16:43 +0200},
	Date-Modified = {2018-09-14 09:17:26 +0200},
	Lastchecked = {Sept. 14, 2018},
	Url = {https://www.camera.lbl.gov/xi-cam-interface}}

@inproceedings{Terraz17,
	Address = {Denver, United States},
	Author = {Terraz, Th{\'e}ophile and Ribes, Alejandro and Fournier, Yvan and Iooss, Bertrand and Raffin, Bruno},
	Booktitle = {{The International Conference for High Performance Computing, Networking, Storage and Analysis (Supercomputing)}},
	Date-Added = {2018-09-14 09:16:18 +0200},
	Date-Modified = {2018-09-14 09:16:20 +0200},
	Hal_Id = {hal-01607479},
	Hal_Version = {v1},
	Keywords = {Sensitivity Analysis ; Multi-run Simulations ; Ensemble Simulation ; Sobol' Index ; In Transit Processing},
	Month = Nov,
	Pages = {1 - 14},
	Pdf = {https://hal.inria.fr/hal-01607479/file/main-Sobol-SC-2017-HALVERSION.pdf},
	Title = {{Melissa: Large Scale In Transit Sensitivity Analysis Avoiding Intermediate Files}},
	Url = {https://hal.inria.fr/hal-01607479},
	Year = {2017},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-01607479}}

@inproceedings{Widanagamaachchi15,
	Author = {Wathsala Widanagamaachchi and Jackie Chen and Pavol Klacansky and Valerio Pascucci and Hemanth Kolla and Ankit Bhagatwala and Peer{-}Timo Bremer},
	Bibsource = {dblp computer science bibliography, https://dblp.org},
	Biburl = {https://dblp.org/rec/bib/conf/ldav/Widanagamaachchi15},
	Booktitle = {5th {IEEE} Symposium on Large Data Analysis and Visualization, {LDAV} 2015, Chicago, IL, USA, October 25-26, 2015},
	Crossref = {LDAV15},
	Date-Added = {2018-09-14 08:24:44 +0200},
	Date-Modified = {2018-09-14 08:24:57 +0200},
	Doi = {10.1109/LDAV.2015.7348066},
	Pages = {9--16},
	Timestamp = {Sun, 21 May 2017 00:19:13 +0200},
	Title = {Tracking features in embedded surfaces: Understanding extinction in turbulent combustion},
	Url = {https://doi.org/10.1109/LDAV.2015.7348066},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/LDAV.2015.7348066}}

@proceedings{LDAV15,
	Bibsource = {dblp computer science bibliography, https://dblp.org},
	Biburl = {https://dblp.org/rec/bib/conf/ldav/2015},
	Date-Added = {2018-09-14 08:24:44 +0200},
	Date-Modified = {2018-09-14 08:24:57 +0200},
	Editor = {Janine Bennett and Hank Childs and Markus Hadwiger},
	Isbn = {978-1-4673-8517-6},
	Publisher = {{IEEE} Computer Society},
	Timestamp = {Fri, 11 Dec 2015 18:11:11 +0100},
	Title = {5th {IEEE} Symposium on Large Data Analysis and Visualization, {LDAV} 2015, Chicago, IL, USA, October 25-26, 2015},
	Url = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=7336675},
	Year = {2015},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=7336675}}

@techreport{Gerber18,
	Author = {Gerber, Richard and Hack, James and Riley, Katherine and Antypas, Katie and Coffey, Richard and Dart, Eli and Straatsma, Tjerk and Wells, Jack and Bard, Deborah and Dosanjh, Sudip and Monga, Inder and Papka, Michael E. and Rotman, Lauren},
	Date-Added = {2018-09-14 06:28:50 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.2172/1417653},
	Institution = {US Department of Energy Office of Science and Technical Information},
	Month = {1},
	Place = {United States},
	Title = {Crosscut report: Exascale Requirements Reviews, March 9--10, 2017 -- Tysons Corner, Virginia. An Office of Science review sponsored by: Advanced Scientific Computing Research, Basic Energy Sciences, Biological and Environmental Research, Fusion Energy Sciences, High Energy Physics, Nuclear Physics},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.2172/1417653}}

@article{Childs07,
	Abstract = {With petascale applications comes petascale data; gleaning knowledge and insight from this large-scale data is widely accepted to be a limiting factor in many fields of scientific endeavor. Large-scale data presents two incredible challenges: scale and complexity. The scale challenge refers to the problem of being able to process tremendous numbers of bytes of data within some time constraint. The complexity challenge is more subtle; petascale data is inherently complex and reducing this data to comprehensible forms is difficult. For the scale challenge, it is often assumed that continued usage of techniques viable at the terascale will provide a recipe for success at the petascale. We argue the opposite: following this path will result in cost prohibitive solutions. Instead, we must pursue 'smarter' techniques, such as in situ and multi-resolution processing, which are well established by a decade's worth of research. The forms of postprocessing are very diverse, but they can be summarized in broad use cases: data exploration, presentation graphics, quantitative analysis, comparative analysis, and visual debugging. Together, these use cases are responsive to the complexity challenge. Moreover, these use cases are responsive to the demands of the user community. Making pretty pictures is just one component of the simulation community's postprocessing needs and all of these needs must be addressed for petascale data. A major focus of this paper is to construct a road map for realizing this broad set of use cases at the petascale, and, further, to do this in the most economical way possible. None of the smart processing techniques we consider are a panacea; none are capable of supporting every use case. However, a software architecture that underpins these processing techniques and that can dynamically select between them will provide an economical way to meet these challenging requirements, and in this paper we discuss such an architecture.},
	Author = {Hank Childs},
	Date-Added = {2018-09-14 06:27:24 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Journal = {Journal of Physics: Conference Series},
	Number = {1},
	Pages = {012012},
	Title = {Architectural challenges and solutions for petascale postprocessing},
	Url = {http://stacks.iop.org/1742-6596/78/i=1/a=012012},
	Volume = {78},
	Year = {2007},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/78/i=1/a=012012}}

@inproceedings{Dreher14,
	Author = {M. Dreher and B. Raffin},
	Booktitle = {2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
	Date-Added = {2018-09-14 05:56:22 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.1109/CCGrid.2014.92},
	Keywords = {data analysis;electronic data interchange;input-output programs;molecular dynamics method;natural sciences computing;parallel processing;workflow management software;scientific simulation;high performance computing system;deep memory hierarchy;I/O capability;simulation code;staging nodes;software environments;analytics workflows;processing components;dataflow graph;Python script;target architecture;simulation execution;component coordination;zero-copy intranode communication;internodes data transfers;per-node distributed daemon;molecular dynamics system;Gromacs;simulation performance;Quick surf algorithm;asynchronous in transit analytics;asynchronous in situ analytics;Analytical models;Computational modeling;Data models;Ports (Computers);Numerical models;Standards;Data visualization;n Situ Analytics and Visualization;IO;Molecular Dynamics},
	Month = {May},
	Pages = {277-286},
	Title = {A Flexible Framework for Asynchronous in Situ and in Transit Analytics for Scientific Simulations},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/CCGrid.2014.92}}

@inproceedings{Dorier17,
	Acmid = {3149396},
	Address = {New York, NY, USA},
	Author = {Dorier, Matthieu and Dreher, Matthieu and Peterka, Tom and Ross, Robert},
	Booktitle = {Proceedings of the 2Nd Joint International Workshop on Parallel Data Storage \& Data Intensive Scalable Computing Systems},
	Date-Added = {2018-09-14 05:55:49 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.1145/3149393.3149396},
	Isbn = {978-1-4503-5134-8},
	Keywords = {CoSS, HPC, I/O, contract, data model, metadata, storage},
	Location = {Denver, Colorado},
	Numpages = {6},
	Pages = {13--18},
	Publisher = {ACM},
	Series = {PDSW-DISCS '17},
	Title = {CoSS: Proposing a Contract-based Storage System for HPC},
	Url = {http://doi.acm.org/10.1145/3149393.3149396},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3149393.3149396},
	Bdsk-Url-2 = {https://doi.org/10.1145/3149393.3149396}}

@inproceedings{Wozniak13,
	Author = {J. M. Wozniak and T. G. Armstrong and M. Wilde and D. S. Katz and E. Lusk and I. T. Foster},
	Booktitle = {2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
	Date-Added = {2018-09-14 05:55:08 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.1109/CCGrid.2013.99},
	Keywords = {C++ language;distributed memory systems;FORTRAN;natural sciences computing;optimising compilers;parallel processing;program debugging;Swift-T solution;large-scale application composition;distributed-memory dataflow processing;scientific applications;high-end computing systems;data dependency;programming language;task-parallel applications;high-level optimizing compiler;user programming;user debugging;C-C++-Fortran;IBM Blue Gene-P;Blue Gene-Q;Turbines;Runtime;Programming;Computational modeling;Distributed databases;Libraries;Concurrent computing;Swift;MPI;ADLB;Turbine;Swift/T;dataflow},
	Month = {May},
	Pages = {95-102},
	Title = {Swift/T: Large-Scale Application Composition via Distributed-Memory Dataflow Processing},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/CCGrid.2013.99}}

@article{Dreher17,
	Abstractnote = {Decaf is a dataflow system for the parallel communication of coupled tasks in an HPC workflow. The dataflow can perform arbitrary data transformations ranging from simply forwarding data to complex data redistribution. Decaf does this by allowing the user to allocate resources and execute custom code in the dataflow. All communication through the dataflow is efficient parallel message passing over MPI. The runtime for calling tasks is entirely message-driven; Decaf executes a task when all messages for the task have been received. Such a messagedriven runtime allows cyclic task dependencies in the workflow graph, for example, to enact computational steering based on the result of downstream tasks. Decaf includes a simple Python API for describing the workflow graph. This allows Decaf to stand alone as a complete workflow system, but Decaf can also be used as the dataflow layer by one or more other workflow systems to form a heterogeneous task-based computing environment. In one experiment, we couple a molecular dynamics code with a visualization tool using the FlowVR and Damaris workflow systems and Decaf for the dataflow. In another experiment, we test the coupling of a cosmology code with Voronoi tessellation and density estimation codes using MPI for the simulation, the DIY programming model for the two analysis codes, and Decaf for the dataflow. Such workflows consisting of heterogeneous software infrastructures exist because components are developed separately with different programming models and runtimes, and this is the first time that such heterogeneous coupling of diverse components was demonstrated in situ on HPC systems.},
	Author = {Dreher, M. and Peterka, T.},
	Date-Added = {2018-09-14 05:54:27 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.2172/1372113},
	Month = {7},
	Place = {United States},
	Title = {Decaf: Decoupled Dataflows for In Situ High-Performance Workflows},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.2172/1372113}}

@inproceedings{Ayachit16,
	Author = {U. Ayachit and B. Whitlock and M. Wolf and B. Loring and B. Geveci and D. Lonie and E. W. Bethel},
	Booktitle = {2016 Second Workshop on In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization (ISAV)},
	Date-Added = {2018-09-14 05:52:33 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.1109/ISAV.2016.013},
	Keywords = {application program interfaces;digital simulation;software portability;software reusability;source code (software);user interfaces;SENSEI generic in situ interface;code portability;code reusability;SENSEI API;simulation code;Data models;Arrays;Instruments;Analytical models;Data visualization;Computational modeling;Bridges;Application programming interfaces;Computer aided analysis;High performance computing;Scientific computing},
	Month = {Nov},
	Pages = {40-44},
	Title = {The SENSEI Generic In Situ Interface},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISAV.2016.013}}

@inproceedings{Boyuka14,
	Author = {D. A. Boyuka and S. Lakshminarasimham and X. Zou and Z. Gong and J. Jenkins and E. R. Schendel and N. Podhorszki and Q. Liu and S. Klasky and N. F. Samatova},
	Booktitle = {2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
	Date-Added = {2018-09-14 05:51:46 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.1109/CCGrid.2014.73},
	Keywords = {data handling;input-output programs;middleware;data transformations;ADIOS;I/O middleware;plug in interface;user-transparency;runtime-configurability;I/O optimizations;read-optimizing transforms;I/O reduction;QLG simulation;leadership-class Titan supercomputer;read performance implications;chunk size;access pattern;transform method opacity;compression method;level-of-detail method;Transforms;Middleware;Data models;Runtime;Layout;Arrays;XML;data transforms;ADIOS;I/O middleware;compression;indexing;level-of-detail;storage layout optimization},
	Month = {May},
	Pages = {256-266},
	Title = {Transparent in Situ Data Transformations in ADIOS},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/CCGrid.2014.73}}

@article{Deelman15,
	Author = {Ewa Deelman and Karan Vahi and Gideon Juve and Mats Rynge and Scott Callaghan and Philip J. Maechling and Rajiv Mayani and Weiwei Chen and Rafael Ferreira da Silva and Miron Livny and Kent Wenger},
	Date-Added = {2018-09-14 05:51:03 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {https://doi.org/10.1016/j.future.2014.10.008},
	Issn = {0167-739X},
	Journal = {Future Generation Computer Systems},
	Keywords = {Scientific workflows, Workflow management system, Pegasus},
	Pages = {17 - 35},
	Title = {Pegasus, a workflow management system for science automation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X14002015},
	Volume = {46},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167739X14002015},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.future.2014.10.008}}

@inbook{Afgan11,
	Abstract = {e-Science focuses on the use of computational tools and resources to analyze large scientific datasets. Performing these analyses often requires running a variety of computational tools specific to a given scientific domain. This places a significant burden on individual researchers for whom simply running these tools may be prohibitively difficult, let alone combining tools into a complete analysis, or acquiring data and appropriate computational resources. This limits the productivity of individual researchers and represents a significant barrier to potential scientific discovery. In order to alleviate researchers from such unnecessary complexities and promote more robust science, we have developed a tool integration framework called Galaxy; Galaxy abstracts individual tools behind a consistent and easy-to-use web interface to enable advanced data analysis that requires no informatics expertise. Furthermore, Galaxy facilitates easy addition of developed tools, thus supporting tool developers, as well as transparent and reproducible communication of computationally intensive analyses. Recently, we have enabled trivial deployment of complete a Galaxy solution on aggregated infrastructures, including cloud computing providers.},
	Address = {London},
	Author = {Afgan, Enis and Goecks, Jeremy and Baker, Dannon and Coraor, Nate and Nekrutenko, Anton and Taylor, James},
	Booktitle = {Guide to e-Science: Next Generation Scientific Research and Discovery},
	Date-Added = {2018-09-14 05:49:57 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.1007/978-0-85729-439-5_6},
	Editor = {Yang, Xiaoyu and Wang, Lizhe and Jie, Wei},
	Isbn = {978-0-85729-439-5},
	Pages = {145--177},
	Publisher = {Springer London},
	Title = {Galaxy: A Gateway to Tools in e-Science},
	Url = {https://doi.org/10.1007/978-0-85729-439-5_6},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-0-85729-439-5_6}}

@article{Jain15,
	Abstract = {Summary This paper introduces FireWorks, a workflow software for running high-throughput calculation workflows at supercomputing centers. FireWorks has been used to complete over 50 million CPU-hours worth of computational chemistry and materials science calculations at the National Energy Research Supercomputing Center. It has been designed to serve the demanding high-throughput computing needs of these applications, with extensive support for (i) concurrent execution through job packing, (ii) failure detection and correction, (iii) provenance and reporting for long-running projects, (iv) automated duplicate detection, and (v) dynamic workflows (i.e., modifying the workflow graph during runtime). We have found that these features are highly relevant to enabling modern data-driven and high-throughput science applications, and we discuss our implementation strategy that rests on Python and NoSQL databases (MongoDB). Finally, we present performance data and limitations of our approach along with planned future work. Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
	Author = {Jain, Anubhav and Ong, Shyue Ping and Chen, Wei and Medasani, Bharat and Qu, Xiaohui and Kocher, Michael and Brafman, Miriam and Petretto, Guido and Rignanese, Gian-Marco and Hautier, Geoffroy and Gunter, Daniel and Persson, Kristin A.},
	Date-Added = {2018-09-14 05:48:55 +0200},
	Date-Modified = {2018-09-14 08:09:31 +0200},
	Doi = {10.1002/cpe.3505},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.3505},
	Journal = {Concurrency and Computation: Practice and Experience},
	Keywords = {scientific workflows, high-throughput computing, fault-tolerant computing},
	Number = {17},
	Pages = {5037-5059},
	Title = {FireWorks: a dynamic workflow system designed for high-throughput applications},
	Url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.3505},
	Volume = {27},
	Year = {2015},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.3505},
	Bdsk-Url-2 = {https://doi.org/10.1002/cpe.3505}}
